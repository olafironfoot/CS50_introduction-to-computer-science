{"changed":true,"filter":false,"title":"TweetAnalyzer.py","tooltip":"/pset6/sentiments/TweetAnalyzer.py","value":"import nltk\nimport sys\n#from helpers import get_user_timeline\n\nclass Analyzer():\n    \"\"\"Implements sentiment analysis.\"\"\"\n\n    def __init__(self, positives, negatives):\n        \"\"\"Initialize Analyzer.\"\"\"\n        #variable declarations\n        self.positives = []\n        self.negatives = []      \n        # TODO\n        #load positive and negative words\n        #store those words in positive and negative word files into list into self.positive, self.negative\n        filePositives = open(positives, \"r\")\n        fileNegatives = open(negatives, \"r\")\n        \n        for line in filePositives:\n            #don't include comment, using str.startswith\n            if not(line.startswith(';') or line.startswith('\\n')):\n                #ommit leading & trailing whitespace using str.strip (https://docs.python.org/3/library/stdtypes.html)\n                self.positives.append(line.rstrip(\"\\n\"))\n        filePositives.close()\n        #print(\"{} <-- self.positives[1]\".format(self.positives[2]))\n        \n        for x in fileNegatives:\n            #don't include comment, using str.startswith\n            if not(x.startswith(';') or x.startswith('\\n')):\n                #ommit leading & trailing whitespace using str.strip (https://docs.python.org/3/library/stdtypes.html)\n                self.negatives.append(x.rstrip(\"\\n\"))\n        fileNegatives.close()\n        print(\"{} <-- self.negatives[0]\".format(self.negatives[0]))\n\n#---------- tweetAnalyzer  \n\n        #tokenize the tweet (like we did in analyzer.py for \"smile\")\n        #tokenizers are part of natural language toolkit\n        #use a TweetTokenizer to split into a list of words\n    def TweetAnalyzer(self, tweet):\n        \"\"\"Analyze text for sentiment, returning its score.\"\"\"\n        \n        #declare variable\n        score = 0\n        # TODO\n        #assign each word in text a value\n        #calculate text's total score\n        \n        #use token to separate user text into single word(one word per token)\n        #Iteration\n        #for i in range(len(str.split())):\n            #extract from the sentence a token\n            #token = i.self.tokenizer.tokenize(text)\n        screen_name = sys.argv[1]\n        tweet = get_user_timeline(screen_name, count=200)\n        tokenizer = nltk.tokenize.TweetTokenizer()\n        tokens = tokenizer.tokenize(tweet)\n        \n        for i in tokens:\n        #turn token into lower case\n            wordToCheck = str.lower(i)\n            print(\"{}\".format(wordToCheck))\n    \n    #with self.positives as lines:\n        #for line in lines:\n            if(wordToCheck in self.positives):\n                score =+ 1\n            elif(wordToCheck in self.negatives):\n                score =- 1\n        \n            \n                TODO\n                \n        Zymala example after explaination below\n        Iterate over tokens\n        str.lower \n        \n        check if word is positive or negative\n        if token in self.positive (if not in positive or negative, means neutral)\n\n        return score\n        \n\n    # def analyze(self, text):\n    #     \"\"\"Analyze text for sentiment, returning its score.\"\"\"\n        \n    #     #declare variable\n    #     score = 0\n    #     # TODO\n    #     #assign each word in text a value\n    #     #calculate text's total score\n        \n    #     #use token to separate user text into single word(one word per token)\n    #     #Iteration\n    #     #for i in range(len(str.split())):\n    #         #extract from the sentence a token\n    #         #token = i.self.tokenizer.tokenize(text)\n    #     tokenizer = nltk.tokenize.TweetTokenizer()\n    #     tokens = tokenizer.tokenize(text)\n        \n    #     for i in tokens:\n    #     #turn token into lower case\n    #         wordToCheck = str.lower(i)\n    #         print(\"{}\".format(wordToCheck))\n    \n    # #with self.positives as lines:\n    #     #for line in lines:\n    #         if(wordToCheck in self.positives):\n    #             score =+ 1\n    #         elif(wordToCheck in self.negatives):\n    #             score =- 1\n                \n\n","undoManager":{"mark":-2,"position":7,"stack":[[{"start":{"row":0,"column":0},"end":{"row":112,"column":0},"action":"insert","lines":["import nltk","import sys","#from helpers import get_user_timeline","","class Analyzer():","    \"\"\"Implements sentiment analysis.\"\"\"","","    def __init__(self, positives, negatives):","        \"\"\"Initialize Analyzer.\"\"\"","        #variable declarations","        self.positives = []","        self.negatives = []      ","        # TODO","        #load positive and negative words","        #store those words in positive and negative word files into list into self.positive, self.negative","        filePositives = open(positives, \"r\")","        fileNegatives = open(negatives, \"r\")","        ","        for line in filePositives:","            #don't include comment, using str.startswith","            if not(line.startswith(';') or line.startswith('\\n')):","                #ommit leading & trailing whitespace using str.strip (https://docs.python.org/3/library/stdtypes.html)","                self.positives.append(line.rstrip(\"\\n\"))","        filePositives.close()","        #print(\"{} <-- self.positives[1]\".format(self.positives[2]))","        ","        for x in fileNegatives:","            #don't include comment, using str.startswith","            if not(x.startswith(';') or x.startswith('\\n')):","                #ommit leading & trailing whitespace using str.strip (https://docs.python.org/3/library/stdtypes.html)","                self.negatives.append(x.rstrip(\"\\n\"))","        fileNegatives.close()","        print(\"{} <-- self.negatives[0]\".format(self.negatives[0]))","","        ","","    def analyze(self, text):","        \"\"\"Analyze text for sentiment, returning its score.\"\"\"","        ","        #declare variable","        score = 0","        # TODO","        #assign each word in text a value","        #calculate text's total score","        ","        #use token to separate user text into single word(one word per token)","        #Iteration","        #for i in range(len(str.split())):","            #extract from the sentence a token","            #token = i.self.tokenizer.tokenize(text)","        tokenizer = nltk.tokenize.TweetTokenizer()","        tokens = tokenizer.tokenize(text)","        ","        for i in tokens:","        #turn token into lower case","            wordToCheck = str.lower(i)","            print(\"{}\".format(wordToCheck))","    ","    #with self.positives as lines:","        #for line in lines:","            if(wordToCheck in self.positives):","                score =+ 1","            elif(wordToCheck in self.negatives):","                score =- 1","                ","#---------- tweetAnalyzer  ","","    #     #tokenize the tweet (like we did in analyzer.py for \"smile\")","    #     #tokenizers are part of natural language toolkit","    #     #use a TweetTokenizer to split into a list of words","    # def TweetAnalyzer(self, tweet):","    #     \"\"\"Analyze text for sentiment, returning its score.\"\"\"","        ","    #     #declare variable","    #     score = 0","    #     # TODO","    #     #assign each word in text a value","    #     #calculate text's total score","        ","    #     #use token to separate user text into single word(one word per token)","    #     #Iteration","    #     #for i in range(len(str.split())):","    #         #extract from the sentence a token","    #         #token = i.self.tokenizer.tokenize(text)","    #     screen_name = sys.argv[1]","    #     tweet = get_user_timeline(screen_name, count=200)","    #     tokenizer = nltk.tokenize.TweetTokenizer()","    #     tokens = tokenizer.tokenize(tweet)","        ","    #     for i in tokens:","    #     #turn token into lower case","    #         wordToCheck = str.lower(i)","    #         print(\"{}\".format(wordToCheck))","    ","    # #with self.positives as lines:","    #     #for line in lines:","    #         if(wordToCheck in self.positives):","    #             score =+ 1","    #         elif(wordToCheck in self.negatives):","    #             score =- 1","        ","            ","                #TODO","                ","        #Zymala example after explaination below","        #Iterate over tokens","        #str.lower ","        ","        #check if word is positive or negative","        #if token in self.positive (if not in positive or negative, means neutral)","        #return score","        return score",""],"id":1}],[{"start":{"row":36,"column":4},"end":{"row":36,"column":6},"action":"insert","lines":["# "],"id":4},{"start":{"row":37,"column":4},"end":{"row":37,"column":6},"action":"insert","lines":["# "]},{"start":{"row":39,"column":4},"end":{"row":39,"column":6},"action":"insert","lines":["# "]},{"start":{"row":40,"column":4},"end":{"row":40,"column":6},"action":"insert","lines":["# "]},{"start":{"row":41,"column":4},"end":{"row":41,"column":6},"action":"insert","lines":["# "]},{"start":{"row":42,"column":4},"end":{"row":42,"column":6},"action":"insert","lines":["# "]},{"start":{"row":43,"column":4},"end":{"row":43,"column":6},"action":"insert","lines":["# "]},{"start":{"row":45,"column":4},"end":{"row":45,"column":6},"action":"insert","lines":["# "]},{"start":{"row":46,"column":4},"end":{"row":46,"column":6},"action":"insert","lines":["# "]},{"start":{"row":47,"column":4},"end":{"row":47,"column":6},"action":"insert","lines":["# "]},{"start":{"row":48,"column":4},"end":{"row":48,"column":6},"action":"insert","lines":["# "]},{"start":{"row":49,"column":4},"end":{"row":49,"column":6},"action":"insert","lines":["# "]},{"start":{"row":50,"column":4},"end":{"row":50,"column":6},"action":"insert","lines":["# "]},{"start":{"row":51,"column":4},"end":{"row":51,"column":6},"action":"insert","lines":["# "]},{"start":{"row":53,"column":4},"end":{"row":53,"column":6},"action":"insert","lines":["# "]},{"start":{"row":54,"column":4},"end":{"row":54,"column":6},"action":"insert","lines":["# "]},{"start":{"row":55,"column":4},"end":{"row":55,"column":6},"action":"insert","lines":["# "]},{"start":{"row":56,"column":4},"end":{"row":56,"column":6},"action":"insert","lines":["# "]},{"start":{"row":58,"column":4},"end":{"row":58,"column":6},"action":"insert","lines":["# "]},{"start":{"row":59,"column":4},"end":{"row":59,"column":6},"action":"insert","lines":["# "]},{"start":{"row":60,"column":4},"end":{"row":60,"column":6},"action":"insert","lines":["# "]},{"start":{"row":61,"column":4},"end":{"row":61,"column":6},"action":"insert","lines":["# "]},{"start":{"row":62,"column":4},"end":{"row":62,"column":6},"action":"insert","lines":["# "]},{"start":{"row":63,"column":4},"end":{"row":63,"column":6},"action":"insert","lines":["# "]}],[{"start":{"row":67,"column":4},"end":{"row":67,"column":6},"action":"remove","lines":["# "],"id":5},{"start":{"row":68,"column":4},"end":{"row":68,"column":6},"action":"remove","lines":["# "]},{"start":{"row":69,"column":4},"end":{"row":69,"column":6},"action":"remove","lines":["# "]},{"start":{"row":70,"column":4},"end":{"row":70,"column":6},"action":"remove","lines":["# "]},{"start":{"row":71,"column":4},"end":{"row":71,"column":6},"action":"remove","lines":["# "]},{"start":{"row":73,"column":4},"end":{"row":73,"column":6},"action":"remove","lines":["# "]},{"start":{"row":74,"column":4},"end":{"row":74,"column":6},"action":"remove","lines":["# "]},{"start":{"row":75,"column":4},"end":{"row":75,"column":6},"action":"remove","lines":["# "]},{"start":{"row":76,"column":4},"end":{"row":76,"column":6},"action":"remove","lines":["# "]},{"start":{"row":77,"column":4},"end":{"row":77,"column":6},"action":"remove","lines":["# "]},{"start":{"row":79,"column":4},"end":{"row":79,"column":6},"action":"remove","lines":["# "]},{"start":{"row":80,"column":4},"end":{"row":80,"column":6},"action":"remove","lines":["# "]},{"start":{"row":81,"column":4},"end":{"row":81,"column":6},"action":"remove","lines":["# "]},{"start":{"row":82,"column":4},"end":{"row":82,"column":6},"action":"remove","lines":["# "]},{"start":{"row":83,"column":4},"end":{"row":83,"column":6},"action":"remove","lines":["# "]},{"start":{"row":84,"column":4},"end":{"row":84,"column":6},"action":"remove","lines":["# "]},{"start":{"row":85,"column":4},"end":{"row":85,"column":6},"action":"remove","lines":["# "]},{"start":{"row":86,"column":4},"end":{"row":86,"column":6},"action":"remove","lines":["# "]},{"start":{"row":87,"column":4},"end":{"row":87,"column":6},"action":"remove","lines":["# "]},{"start":{"row":89,"column":4},"end":{"row":89,"column":6},"action":"remove","lines":["# "]},{"start":{"row":90,"column":4},"end":{"row":90,"column":6},"action":"remove","lines":["# "]},{"start":{"row":91,"column":4},"end":{"row":91,"column":6},"action":"remove","lines":["# "]},{"start":{"row":92,"column":4},"end":{"row":92,"column":6},"action":"remove","lines":["# "]},{"start":{"row":94,"column":4},"end":{"row":94,"column":6},"action":"remove","lines":["# "]},{"start":{"row":95,"column":4},"end":{"row":95,"column":6},"action":"remove","lines":["# "]},{"start":{"row":96,"column":4},"end":{"row":96,"column":6},"action":"remove","lines":["# "]},{"start":{"row":97,"column":4},"end":{"row":97,"column":6},"action":"remove","lines":["# "]},{"start":{"row":98,"column":4},"end":{"row":98,"column":6},"action":"remove","lines":["# "]},{"start":{"row":99,"column":4},"end":{"row":99,"column":6},"action":"remove","lines":["# "]},{"start":{"row":102,"column":16},"end":{"row":102,"column":17},"action":"remove","lines":["#"]},{"start":{"row":104,"column":8},"end":{"row":104,"column":9},"action":"remove","lines":["#"]},{"start":{"row":105,"column":8},"end":{"row":105,"column":9},"action":"remove","lines":["#"]},{"start":{"row":106,"column":8},"end":{"row":106,"column":9},"action":"remove","lines":["#"]},{"start":{"row":108,"column":8},"end":{"row":108,"column":9},"action":"remove","lines":["#"]},{"start":{"row":109,"column":8},"end":{"row":109,"column":9},"action":"remove","lines":["#"]},{"start":{"row":110,"column":8},"end":{"row":110,"column":9},"action":"remove","lines":["#"]}],[{"start":{"row":110,"column":0},"end":{"row":111,"column":0},"action":"remove","lines":["        return score",""],"id":6}],[{"start":{"row":110,"column":0},"end":{"row":111,"column":0},"action":"insert","lines":["",""],"id":7}],[{"start":{"row":65,"column":0},"end":{"row":111,"column":20},"action":"remove","lines":["#---------- tweetAnalyzer  ","","        #tokenize the tweet (like we did in analyzer.py for \"smile\")","        #tokenizers are part of natural language toolkit","        #use a TweetTokenizer to split into a list of words","    def TweetAnalyzer(self, tweet):","        \"\"\"Analyze text for sentiment, returning its score.\"\"\"","        ","        #declare variable","        score = 0","        # TODO","        #assign each word in text a value","        #calculate text's total score","        ","        #use token to separate user text into single word(one word per token)","        #Iteration","        #for i in range(len(str.split())):","            #extract from the sentence a token","            #token = i.self.tokenizer.tokenize(text)","        screen_name = sys.argv[1]","        tweet = get_user_timeline(screen_name, count=200)","        tokenizer = nltk.tokenize.TweetTokenizer()","        tokens = tokenizer.tokenize(tweet)","        ","        for i in tokens:","        #turn token into lower case","            wordToCheck = str.lower(i)","            print(\"{}\".format(wordToCheck))","    ","    #with self.positives as lines:","        #for line in lines:","            if(wordToCheck in self.positives):","                score =+ 1","            elif(wordToCheck in self.negatives):","                score =- 1","        ","            ","                TODO","                ","        Zymala example after explaination below","        Iterate over tokens","        str.lower ","        ","        check if word is positive or negative","        if token in self.positive (if not in positive or negative, means neutral)","","        return score"],"id":8}],[{"start":{"row":33,"column":0},"end":{"row":34,"column":0},"action":"insert","lines":["",""],"id":9}],[{"start":{"row":34,"column":0},"end":{"row":80,"column":20},"action":"insert","lines":["#---------- tweetAnalyzer  ","","        #tokenize the tweet (like we did in analyzer.py for \"smile\")","        #tokenizers are part of natural language toolkit","        #use a TweetTokenizer to split into a list of words","    def TweetAnalyzer(self, tweet):","        \"\"\"Analyze text for sentiment, returning its score.\"\"\"","        ","        #declare variable","        score = 0","        # TODO","        #assign each word in text a value","        #calculate text's total score","        ","        #use token to separate user text into single word(one word per token)","        #Iteration","        #for i in range(len(str.split())):","            #extract from the sentence a token","            #token = i.self.tokenizer.tokenize(text)","        screen_name = sys.argv[1]","        tweet = get_user_timeline(screen_name, count=200)","        tokenizer = nltk.tokenize.TweetTokenizer()","        tokens = tokenizer.tokenize(tweet)","        ","        for i in tokens:","        #turn token into lower case","            wordToCheck = str.lower(i)","            print(\"{}\".format(wordToCheck))","    ","    #with self.positives as lines:","        #for line in lines:","            if(wordToCheck in self.positives):","                score =+ 1","            elif(wordToCheck in self.negatives):","                score =- 1","        ","            ","                TODO","                ","        Zymala example after explaination below","        Iterate over tokens","        str.lower ","        ","        check if word is positive or negative","        if token in self.positive (if not in positive or negative, means neutral)","","        return score"],"id":10}]]},"ace":{"folds":[],"scrolltop":305.5,"scrollleft":0,"selection":{"start":{"row":39,"column":3},"end":{"row":39,"column":3},"isBackwards":false},"options":{"guessTabSize":true,"useWrapMode":false,"wrapToView":true},"firstLineState":{"row":18,"state":"start","mode":"ace/mode/python"}},"timestamp":1491418427512}